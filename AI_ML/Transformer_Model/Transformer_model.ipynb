{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zSAISuuAPrMz"
      },
      "outputs": [],
      "source": [
        "'''=====================================\n",
        "Multi-Head Attention Module (PyTorch)\n",
        "====================================='''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # QKV projection\n",
        "        qkv = self.qkv_proj(x)  # (B, T, 3*embed_dim)\n",
        "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, T, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = attn @ v  # (B, heads, T, head_dim)\n",
        "\n",
        "        # Merge heads\n",
        "        out = out.transpose(1, 2).reshape(B, T, C)\n",
        "        return self.out_proj(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''=====================================\n",
        "Positional Encoding Module (PyTorch)\n",
        "====================================='''\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        positions = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "        return x + self.pos_embedding(positions)\n"
      ],
      "metadata": {
        "id": "OasVnjmGPzax"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fake input: batch_size=2, seq_len=5, vocab_size=100\n",
        "x = torch.randint(0, 100, (2, 5))\n",
        "embed_dim = 32\n",
        "num_heads = 4\n",
        "max_len = 10\n",
        "\n",
        "# Token embedding (for testing)\n",
        "token_emb = nn.Embedding(100, embed_dim)\n",
        "x_emb = token_emb(x)\n",
        "\n",
        "# Add positional encoding\n",
        "pos_enc = PositionalEncoding(max_len, embed_dim)\n",
        "x_emb = pos_enc(x_emb)\n",
        "\n",
        "# Multi-head attention\n",
        "mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "out = mha(x_emb)  # output shape: (2, 5, 32)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTHzqNivQI3Y",
        "outputId": "2e2fe9ac-fbde-4c82-a357-23d5c7fc622e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 32])\n"
          ]
        }
      ]
    }
  ]
}